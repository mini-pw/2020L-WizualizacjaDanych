{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MGU - projekt 1a.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvnbleid/2020L-WizualizacjaDanych/blob/master/MGU_projekt_1a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75gFz93Qm-_0",
        "colab_type": "text"
      },
      "source": [
        "**Podpięcie Google Drive:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClpqKdHrl9_r",
        "colab_type": "code",
        "outputId": "64c18131-b598-4529-ecca-e628e4df0436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "BASE_DIR = '/content/gdrive/My Drive/DL2020/Projekt1a'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvtvPF6AlkGl",
        "colab_type": "text"
      },
      "source": [
        "**Funkcje aktywacji:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7dQgHyMlXQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "\n",
        "def relu_prime(x):\n",
        "    return np.where(x > 0, 1.0, 0.0)\n",
        "\n",
        "\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def identity_prime(x):\n",
        "    return 1.0\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1.0 - np.square(np.tanh(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc5ePKTZlz91",
        "colab_type": "text"
      },
      "source": [
        "**Ładowanie danych:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ETJvtvdliGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "def load_classification(csv_filename, argument_column_names, class_column_name='cls'):\n",
        "    df = pd.read_csv(csv_filename)\n",
        "\n",
        "    output_layer_neuron_count = len(df[class_column_name].unique())\n",
        "    input_layer_neuron_count = len(argument_column_names)\n",
        "\n",
        "    training_data = []\n",
        "\n",
        "    for row in df.itertuples():\n",
        "        x = np.zeros((input_layer_neuron_count, 1))\n",
        "\n",
        "        for i in range(len(argument_column_names)):\n",
        "            x[i] = getattr(row, argument_column_names[i])\n",
        "\n",
        "        y = vectorize_class(getattr(row, class_column_name),\n",
        "                            output_layer_neuron_count)\n",
        "\n",
        "        training_data.append((x, y))\n",
        "    return training_data\n",
        "\n",
        "\n",
        "def load_regression(csv_filename, input_column_names, output_column_names):\n",
        "    df = pd.read_csv(csv_filename)\n",
        "\n",
        "    output_layer_neuron_count = len(output_column_names)\n",
        "    input_layer_neuron_count = len(input_column_names)\n",
        "\n",
        "    training_data = []\n",
        "\n",
        "    for row in df.itertuples():\n",
        "        x = np.zeros((input_layer_neuron_count, 1))\n",
        "        y = np.zeros((output_layer_neuron_count, 1))\n",
        "\n",
        "        for i in range(len(input_column_names)):\n",
        "            x[i] = getattr(row, input_column_names[i])\n",
        "\n",
        "        for i in range(len(output_column_names)):\n",
        "            y[i] = getattr(row, output_column_names[i])\n",
        "\n",
        "        training_data.append((x, y))\n",
        "    return training_data\n",
        "\n",
        "\n",
        "def load_classification_wrapper(path, name, train_size, test_size=None):\n",
        "    if test_size == None:\n",
        "        test_size = train_size\n",
        "\n",
        "    train = load_classification(os.path.join(\n",
        "        path, f'{name}.train.{train_size}.csv'), ['x', 'y'])\n",
        "    test = load_classification(os.path.join(\n",
        "        path, f'{name}.test.{test_size}.csv'), ['x', 'y'])\n",
        "    return (train, test)\n",
        "\n",
        "\n",
        "def load_regression_wrapper(path, name, train_size, test_size=None):\n",
        "    if test_size == None:\n",
        "        test_size = train_size\n",
        "\n",
        "    train = load_regression(os.path.join(\n",
        "        path, f'{name}.train.{train_size}.csv'), ['x'], ['y'])\n",
        "    test = load_regression(os.path.join(\n",
        "        path, f'{name}.test.{test_size}.csv'), ['x'], ['y'])\n",
        "    return (train, test)\n",
        "\n",
        "\n",
        "def vectorize_class(class_id, class_length):\n",
        "    # assume that classification starts at 1 to class_length, e.g. 1,2,3,4\n",
        "    class_id = class_id - 1\n",
        "    y = np.zeros((class_length, 1))\n",
        "    y[class_id] = 1\n",
        "    return y\n",
        "\n",
        "def load_data(hidden_layer_sizes, hidden_layers_activation_function, hidden_layers_activation_function_prime, output_layer_activation_function, output_layer_activation_function_prime, cost_derivative, is_bias_enabled, path, name, train_size, test_size=None):\n",
        "    train_data, test_data = load_classification_wrapper(\n",
        "        path, name, train_size, test_size)\n",
        "    sizes = [len(train_data[0][0])]\n",
        "    sizes.extend(hidden_layer_sizes)\n",
        "    sizes.append(len(train_data[0][1]))\n",
        "    return train_data, test_data, Network(sizes, hidden_layers_activation_function, hidden_layers_activation_function_prime, output_layer_activation_function, output_layer_activation_function_prime, cost_derivative, is_bias_enabled)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRVZDz6tgz2B",
        "colab_type": "text"
      },
      "source": [
        "**Pochodne funkcji straty:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8JLKM62g428",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def quadratic_cost_derivative(output_activations, y):\n",
        "    return (output_activations - y)\n",
        "\n",
        "def cross_entropy_cost_derivative(output_activations, y):\n",
        "    return (output_activations - y)/((1-output_activations) * output_activations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG44ZAlYl7Sn",
        "colab_type": "text"
      },
      "source": [
        "**Architektura sieci:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk1YClANo1ZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import codecs\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "class Problem(Enum):\n",
        "    Classification = 1\n",
        "    Regression = 2\n",
        "\n",
        "\n",
        "class Network:\n",
        "\n",
        "    def __init__(self, sizes, hidden_layers_activation_function, hidden_layers_activation_function_prime, output_layer_activation_function, output_layer_activation_function_prime, cost_derivative, is_bias_enabled):\n",
        "        self.hidden_layers_activation_function = hidden_layers_activation_function\n",
        "        self.hidden_layers_activation_function_prime = hidden_layers_activation_function_prime\n",
        "        self.output_layer_activation_function = output_layer_activation_function\n",
        "        self.output_layer_activation_function_prime = output_layer_activation_function_prime\n",
        "        self.cost_derivative = cost_derivative\n",
        "        self.sizes = sizes\n",
        "        self.num_layers = len(sizes)\n",
        "        self.is_bias_enabled = is_bias_enabled\n",
        "        self.eta = None\n",
        "        self.epochs = None\n",
        "        self.batch_size = None\n",
        "        self.training_data_size = None\n",
        "        self.problem = None\n",
        "        self.history = []\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "        self.classification_area_probing = []\n",
        "        self.prev_nabla_w = None\n",
        "        self.prev_nabla_b = None\n",
        "        self.nabla_w = None\n",
        "        self.nabla_b = None\n",
        "\n",
        "    def disable_bias(self):\n",
        "        self.biases = [np.zeros((y, 1)) for y in self.sizes[1:]]\n",
        "        self.is_bias_enabled = False\n",
        "\n",
        "    def change_hidden_layer_activation_function(self, activation_function, activation_function_prime):\n",
        "        self.hidden_layers_activation_function = activation_function\n",
        "        self.hidden_layers_activation_function_prime = activation_function_prime\n",
        "\n",
        "    def change_output_layer_activation_function(self, activation_function, activation_function_prime):\n",
        "        self.output_layer_activation_function = activation_function\n",
        "        self.output_layer_activation_function_prime = activation_function_prime\n",
        "\n",
        "    def feed_forward(self, a):\n",
        "        for bias, weight in zip(self.biases[:-1], self.weights[:-1]):\n",
        "            a = self.hidden_layers_activation_function(\n",
        "                np.dot(weight, a) + bias)\n",
        "        return self.output_layer_activation_function(np.dot(self.weights[-1], a) + self.biases[-1])\n",
        "\n",
        "    def backpropagate(self, x, y):\n",
        "        nabla_b = [np.zeros(bias.shape) for bias in self.biases]\n",
        "        nabla_w = [np.zeros(weight.shape) for weight in self.weights]\n",
        "\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "\n",
        "        # feed forward hidden layers\n",
        "        for bias, weight in zip(self.biases[:-1], self.weights[:-1]):\n",
        "            z = np.dot(weight, activation) + bias\n",
        "            zs.append(z)\n",
        "            activation = self.hidden_layers_activation_function(z)\n",
        "            activations.append(activation)\n",
        "        # feed forward output layer\n",
        "        z = np.dot(self.weights[-1], activation) + self.biases[-1]\n",
        "        zs.append(z)\n",
        "        activation = self.output_layer_activation_function(z)\n",
        "        activations.append(activation)\n",
        "\n",
        "        derivative = self.cost_derivative(activations[-1], y)\n",
        "        delta = derivative * \\\n",
        "            self.output_layer_activation_function_prime(zs[-1])\n",
        "\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "        for layer_id in range(2, self.num_layers):\n",
        "            z = zs[-layer_id]\n",
        "            afp = self.hidden_layers_activation_function_prime(z)\n",
        "            delta = np.dot(self.weights[-layer_id+1].transpose(), delta) * afp\n",
        "            nabla_b[-layer_id] = delta\n",
        "            nabla_w[-layer_id] = np.dot(delta,\n",
        "                                        activations[-layer_id - 1].transpose())\n",
        "\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    def run_batch(self, batch, eta, alpha=0):\n",
        "        self.prev_nabla_b = self.nabla_b\n",
        "        self.prev_nabla_w = self.nabla_w\n",
        "\n",
        "        self.nabla_b = [np.zeros(bias.shape) for bias in self.biases]\n",
        "        self.nabla_w = [np.zeros(weight.shape) for weight in self.weights]\n",
        "\n",
        "        for x, y in batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backpropagate(x, y)\n",
        "\n",
        "            self.nabla_b = [nb + dnb for nb,\n",
        "                            dnb in zip(self.nabla_b, delta_nabla_b)]\n",
        "            self.nabla_w = [nw + dnw for nw,\n",
        "                            dnw in zip(self.nabla_w, delta_nabla_w)]\n",
        "\n",
        "        eta_normalized = eta/len(batch)\n",
        "        alpha_normalzied = alpha/len(batch)\n",
        "\n",
        "        if self.prev_nabla_w == None:\n",
        "            self.weights = [w - eta_normalized * nw\n",
        "                            for w, nw in zip(self.weights, self.nabla_w)]\n",
        "        else:\n",
        "            self.weights = [w - eta_normalized * nw - alpha_normalzied * pnw\n",
        "                            for w, nw, pnw in zip(self.weights, self.nabla_w, self.prev_nabla_w)]\n",
        "\n",
        "        if self.is_bias_enabled:\n",
        "            if self.prev_nabla_b == None:\n",
        "                self.biases = [b - eta_normalized * nb\n",
        "                               for b, nb in zip(self.biases, self.nabla_b)]\n",
        "            else:\n",
        "                self.biases = [b - eta_normalized * nb - alpha_normalzied * pnb\n",
        "                               for b, nb, pnb in zip(self.biases, self.nabla_b, self.prev_nabla_b)]\n",
        "\n",
        "    def validate_classification(self, validation_data):\n",
        "        correct = 0\n",
        "        for x, y in validation_data:\n",
        "            expected = np.argmax(y)\n",
        "            result = np.argmax(self.feed_forward(x))\n",
        "            if expected == result:\n",
        "                correct = correct + 1\n",
        "        return correct / len(validation_data)\n",
        "\n",
        "    def validate_regression(self, validation_data):\n",
        "        error = 0\n",
        "        for x, y in validation_data:\n",
        "            error = error + np.square(self.feed_forward(x)-y).mean()\n",
        "        return error / len(validation_data)\n",
        "\n",
        "    def stochastic_gradient_descent_classification(self, training_data, epochs, batch_size, eta,\n",
        "                                                   test_data=None, probe=None, alpha=0):\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.eta = eta\n",
        "        self.training_data_size = len(training_data)\n",
        "        self.take_snapshot(f'Start {Problem.Classification}', None)\n",
        "        self.problem = Problem.Classification\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            np.random.shuffle(training_data)\n",
        "            batches = [training_data[i:i+batch_size] for\n",
        "                       i in range(0, self.training_data_size, batch_size)]\n",
        "            for batch, batch_id in zip(batches, range(1, 1 + len(batches))):\n",
        "                self.run_batch(batch, eta, alpha)\n",
        "                train_fitness = self.validate_classification(training_data)\n",
        "                test_fitness = None\n",
        "                if test_data != None:\n",
        "                    test_fitness = self.validate_classification(test_data)\n",
        "\n",
        "                self.take_snapshot(\n",
        "                    f'Epoch {epoch}: [{batch_id}/{len(batches)}]', train_fitness=train_fitness, test_fitness=test_fitness)\n",
        "            if probe != None:\n",
        "                self.classification_area_probing.append(\n",
        "                    self.classificate(probe))\n",
        "\n",
        "    def stochastic_gradient_descent_regression(self, training_data, epochs, batch_size, eta,\n",
        "                                               validation_data=None):\n",
        "        self.__stochastic_gradient_descent(\n",
        "            training_data, epochs, batch_size, eta, Problem.Regression, validation_data)\n",
        "\n",
        "    def __stochastic_gradient_descent(self, training_data, epochs, batch_size, eta, problem,\n",
        "                                      validation_data=None):\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.eta = eta\n",
        "        self.training_data_size = len(training_data)\n",
        "        self.take_snapshot(f'Start {problem}', None)\n",
        "        self.problem = problem\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            batches = [training_data[i:i+batch_size] for\n",
        "                       i in range(0, self.training_data_size, batch_size)]\n",
        "            for batch, batch_id in zip(batches, range(1, 1 + len(batches))):\n",
        "                self.run_batch(batch, eta)\n",
        "                fitness = None\n",
        "                if validation_data != None:\n",
        "                    if problem == Problem.Classification:\n",
        "                        fitness = self.validate_classification(validation_data)\n",
        "                    if problem == Problem.Regression:\n",
        "                        fitness = self.validate_regression(validation_data)\n",
        "                self.take_snapshot(\n",
        "                    f'Epoch {epoch}: [{batch_id}/{len(batches)}]', fitness)\n",
        "\n",
        "    def take_snapshot(self, name, train_fitness=None, test_fitness=None):\n",
        "        dump = dict(biases=[bias.tolist() for bias in self.biases],\n",
        "                    weights=[weight.tolist() for weight in self.weights],\n",
        "                    name=name,\n",
        "                    train_fitness=train_fitness,\n",
        "                    test_fitness=test_fitness)\n",
        "        self.history.append(dump)\n",
        "\n",
        "    def save_history(self, out_filepath):\n",
        "        network_info = dict(sizes=self.sizes, eta=self.eta,\n",
        "                            epochs=self.epochs, batch_size=self.batch_size,\n",
        "                            training_data_size=self.training_data_size,\n",
        "                            hidden_layers_activation_function=self.hidden_layers_activation_function.__name__,\n",
        "                            output_layer_activation_function=self.output_layer_activation_function.__name__)\n",
        "        json.dump(dict(history=self.history, network_info=network_info),\n",
        "                  codecs.open(out_filepath, 'w', encoding='utf-8'))\n",
        "\n",
        "    def classificate(self, test_data):\n",
        "        results = []\n",
        "        for x in test_data:\n",
        "            classification = np.argmax(self.feed_forward(x))\n",
        "            results.append(classification)\n",
        "        return results\n",
        "\n",
        "    def regression_result(self, test_data):\n",
        "        results = []\n",
        "        for x in test_data:\n",
        "            regression = self.feed_forward(x)\n",
        "            results.append(regression)\n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOsPcRr1t3SG",
        "colab_type": "text"
      },
      "source": [
        "**Uruchamianie:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hx-MF3gt7Nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, test_data, net = load_data([5], sigmoid, sigmoid_prime, sigmoid, sigmoid_prime, quadratic_cost_derivative, True, BASE_DIR + r'/data/classification', 'data.simple', 100)\n",
        "net.stochastic_gradient_descent_classification(\n",
        "    train_data, 100, 10, 0.03, test_data)\n",
        "net.save_history(BASE_DIR + '/results/classification.json')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFASB6ipo8tc",
        "colab_type": "text"
      },
      "source": [
        "**Klasyfikacja - wykresy błędu i wizualizacja rezultatów**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAJ6WW8Yklba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ8fovF2kkp5",
        "colab_type": "text"
      },
      "source": [
        "**Regresja - wykresy błędu i wizualizacja rezultatów**"
      ]
    }
  ]
}